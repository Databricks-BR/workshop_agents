{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21716c9b-7ded-47af-ada6-67dd60a848c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/Databricks-BR/workshop_agents/refs/heads/main/demo-main/img/header_workshop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36fac79b-b82c-45d3-ae73-c9c1c76d4656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " Item | Description |\n",
    " --- | --- |\n",
    " **Objective** | Create an Agent Supervisor with Agent Bricks |\n",
    " **Databricks Run Time** | DBR 16.4 LTS |\n",
    " **Language** | Python and Pyspark |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e63da99e-4561-4ed9-84d4-12882d61ac39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/Databricks-BR/workshop_agents/refs/heads/main/demo-main/img/img/06_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe5b2b18-b25a-476a-8cdf-8f7dee06134d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create an Agent Supervisor with Agent Bricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e00a836c-61ae-4048-801b-4200fd7838b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/Databricks-BR/workshop_agents/refs/heads/main/demo-main/img/img/07_agent_bricks.png)\n",
    "\n",
    "**Agent Bricks** is a streamlined platform designed to make deploying, managing, and scaling intelligent agents remarkably simple for every organization. With a focus on low-friction, user-friendly workflows, Agent Bricks empowers teams to quickly build, customize, and operationalize AI-powered agents tailored to diverse business needs—without the complexity of traditional software development. Its modular, flexible approach enables customers to prototype, iterate, and launch powerful automation or analytics agents with minimal overhead, accelerating innovation and reducing time-to-value. Whether you’re looking to enhance customer support, automate data workflows, or unlock new AI-driven insights, Agent Bricks provides an intuitive foundation for rapid experimentation and effortless deployment of trusted AI solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6b81e5-9e65-4155-8bc0-806c7c83e306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read pdf file from /Volumes/vinicius_fialho_testes/workshop_ml_agentes/unstructured_data/meli-2025-08-04-letter.pdf and use ai_parse_document to extract\n",
    "from pyspark.sql.functions import expr\n",
    "df=spark.read.format(\"binaryFile\").load(\"/Volumes/vinicius_fialho_testes/workshop_ml_agentes/unstructured_data/meli-2025-08-04-letter.pdf\").withColumn(\n",
    "\n",
    "\n",
    "    \"parsed\",\n",
    "    expr(\"ai_parse_document(content)\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25084b06-072b-4371-95ae-a91294c6b142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extract all columns from dataframe\n",
    "from pyspark.sql.functions import col, parse_json\n",
    "\n",
    "df_copy=df.withColumn(\n",
    "   \"parsed_json\",\n",
    "   parse_json(col(\"parsed\").cast(\"string\"))) \\\n",
    " .selectExpr(\n",
    "   \"path\",\n",
    "   \"parsed_json:document:elements\")\n",
    "display(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f06fe29-839a-4766-a4a5-1d2eaf2f3f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Explode into multiple rows with content from each page\n",
    "from pyspark.sql.functions import explode, from_json\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "\n",
    "# Define the expected array type for your data\n",
    "array_schema = ArrayType(StringType())\n",
    "\n",
    "# Convert 'elements' (VARIANT) to array by parsing as JSON string\n",
    "df_copy2 = df_copy.withColumn(\"elements_array\", from_json(col(\"elements\").cast(\"string\"), array_schema))\n",
    "\n",
    "# Explode the new array column\n",
    "df_copy3 = df_copy2.select(\"path\", explode(col(\"elements_array\")).alias(\"element\"))\n",
    "display(df_copy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d1515a-aa66-4747-b538-52f48f28fc29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_copy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31eed4e-b09c-466c-b70c-cb141ccf7096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#Add id to the dataframe\n",
    "df_copy4=df_copy3.withColumn(\"id\", monotonically_increasing_id())\n",
    "display(df_copy4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1758a255-7709-4a19-8e68-82027b51ee01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write dataframe to table vinicius_fialho_testes.workshop_ml_agentes.document_meli\n",
    "df_copy4.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"vinicius_fialho_testes.workshop_ml_agentes.document_meli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "081243e6-05fc-42ba-84a4-0db1e31bc92f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agente: \n",
    "https://e2-demo-field-eng.cloud.databricks.com/ml/bricks/mas/configure/b6af9a59-b66b-4d32-8011-8f9c2fa29d4d?o=1444828305810485\n",
    "\n",
    "* ¿Que productos tienen un inventario inferior a 200 unidades?\n",
    "\n",
    "* ¿Cual es la tasa de crecimiento de Brasil YoY en dolares en Q2 del 2025 para el revenue consolidado?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "05_agent_bricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
